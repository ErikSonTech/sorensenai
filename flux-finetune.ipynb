{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3ee6dc33-60a0-4b86-b641-88d54a913cc4",
      "metadata": {
        "id": "3ee6dc33-60a0-4b86-b641-88d54a913cc4"
      },
      "source": [
        "## Workspace Setup\n",
        "\n",
        "To begin, we need to install the necessary packages for the project. Execute the code in the following cell in your terminal (not in a Jupyter cell) to install all required dependencies. After that, use the second code cell to log in with your Hugging Face credentials, as the models we will use are hosted on Hugging Face.\n",
        "\n",
        "### Copy and paste the following commands into your terminal:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1150a1e3",
      "metadata": {
        "id": "1150a1e3"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ostris/ai-toolkit.git\n",
        "%cd ai-toolkit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8a0065d",
      "metadata": {
        "id": "a8a0065d"
      },
      "outputs": [],
      "source": [
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c944e634",
      "metadata": {
        "id": "c944e634"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9536caea",
      "metadata": {
        "id": "9536caea"
      },
      "outputs": [],
      "source": [
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff5a1566",
      "metadata": {
        "id": "ff5a1566"
      },
      "outputs": [],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b28616e6-63b6-4826-9f63-999bf55dcb57",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-21T21:18:00.107478Z",
          "iopub.status.busy": "2024-08-21T21:18:00.107226Z"
        },
        "id": "b28616e6-63b6-4826-9f63-999bf55dcb57"
      },
      "outputs": [],
      "source": [
        "!git submodule update --init --recursive\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6614b901",
      "metadata": {
        "id": "6614b901"
      },
      "source": [
        "\n",
        "## Image Captioning / Labelling\n",
        "\n",
        "Once the environment is set up, we will caption our images. You need to upload your images into a single directory. Specify the directory name in the variable `your_dir` in the next code cell. Running the cells will automatically generate labeled text files for each image.\n",
        "\n",
        "## Install Additional Packages\n",
        "\n",
        "Before proceeding, ensure you have the following packages installed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1612c3f9-902b-43fd-bcc2-942a20777369",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-21T21:40:28.311150Z",
          "iopub.status.busy": "2024-08-21T21:40:28.310644Z",
          "iopub.status.idle": "2024-08-21T21:40:39.592701Z",
          "shell.execute_reply": "2024-08-21T21:40:39.591997Z",
          "shell.execute_reply.started": "2024-08-21T21:40:28.311130Z"
        },
        "id": "1612c3f9-902b-43fd-bcc2-942a20777369"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install -U transformers\n",
        "!pip install einops\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c72e61a-c28b-4557-b1a7-25da06618fa1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-21T21:40:52.626680Z",
          "iopub.status.busy": "2024-08-21T21:40:52.626119Z",
          "iopub.status.idle": "2024-08-21T21:40:54.079886Z",
          "shell.execute_reply": "2024-08-21T21:40:54.079257Z",
          "shell.execute_reply.started": "2024-08-21T21:40:52.626658Z"
        },
        "tags": [],
        "id": "0c72e61a-c28b-4557-b1a7-25da06618fa1"
      },
      "outputs": [],
      "source": [
        "from unittest.mock import patch\n",
        "from transformers.dynamic_module_utils import get_imports\n",
        "import os\n",
        "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
        "    if not str(filename).endswith(\"modeling_florence2.py\"):\n",
        "        return get_imports(filename)\n",
        "    imports = get_imports(filename)\n",
        "    imports.remove(\"flash_attn\")\n",
        "    return imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b668d04-cfd6-4d02-aad3-ca7135aaa7d8",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-21T21:40:54.610485Z",
          "iopub.status.busy": "2024-08-21T21:40:54.609940Z",
          "iopub.status.idle": "2024-08-21T21:41:33.509539Z",
          "shell.execute_reply": "2024-08-21T21:41:33.508768Z",
          "shell.execute_reply.started": "2024-08-21T21:40:54.610464Z"
        },
        "tags": [],
        "id": "9b668d04-cfd6-4d02-aad3-ca7135aaa7d8"
      },
      "outputs": [],
      "source": [
        "!pip install -U oyaml transformers einops albumentations python-dotenv\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import os\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "\n",
        "\n",
        "model_id = 'microsoft/Florence-2-large'\n",
        "with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports): #workaround for unnecessary flash_attn requirement\n",
        "            model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=\"sdpa\", torch_dtype='auto',trust_remote_code=True)\n",
        "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
        "model.to(device)\n",
        "\n",
        "your_dir = 'fold'\n",
        "\n",
        "\n",
        "prompt = \"<MORE_DETAILED_CAPTION>\"\n",
        "for i in os.listdir(f'{your_dir}'):\n",
        "    if i.split('.')[-1]=='txt':\n",
        "        continue\n",
        "    image = Image.open(f'{your_dir}/'+i)\n",
        "\n",
        "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(device, torch_dtype)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "      input_ids=inputs[\"input_ids\"],\n",
        "      pixel_values=inputs[\"pixel_values\"],\n",
        "      max_new_tokens=1024,\n",
        "      num_beams=3,\n",
        "      do_sample=False\n",
        "    )\n",
        "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "    parsed_answer = processor.post_process_generation(generated_text, task=\"<MORE_DETAILED_CAPTION>\", image_size=(image.width, image.height))\n",
        "    # print(parsed_answer)\n",
        "    with open(f'{your_dir}/'+f\"{i.split('.')[0]}.txt\", \"w\") as f:\n",
        "        f.write(parsed_answer[\"<MORE_DETAILED_CAPTION>\"])\n",
        "        f.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8d3540",
      "metadata": {
        "id": "af8d3540"
      },
      "source": [
        "\n",
        "## Organizing Your Image Folder\n",
        "\n",
        "Ensure your image folder is structured as follows:\n",
        "\n",
        "Your Image Directory\n",
        "│\n",
        "├── img1.png\n",
        "├── img1.txt\n",
        "├── img2.png\n",
        "├── img2.txt\n",
        "└──\n",
        "\n",
        "Make sure that each image has a corresponding text file with the same name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87f6d6a9",
      "metadata": {
        "id": "87f6d6a9"
      },
      "source": [
        "\n",
        "## Configure the YAML File for Training\n",
        "\n",
        "We will use the `config/examples/train_lora_flux_24gb.yaml` file to configure the model training. Important lines to edit include:\n",
        "\n",
        "- **Line 5**: Change the name of the model.\n",
        "- **Line 30**: Specify the path to your image directory.\n",
        "- **Lines 69-70**: Adjust the height and width to match your training images.\n",
        "\n",
        "You may also want to modify the prompts to better suit your training data. Adjust the batch size and gradient accumulation steps for optimal training performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "37744dd5",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-21T21:41:39.617226Z",
          "iopub.status.busy": "2024-08-21T21:41:39.616631Z",
          "iopub.status.idle": "2024-08-21T21:41:39.629149Z",
          "shell.execute_reply": "2024-08-21T21:41:39.628596Z",
          "shell.execute_reply.started": "2024-08-21T21:41:39.617205Z"
        },
        "id": "37744dd5"
      },
      "outputs": [],
      "source": [
        "your_dir = '/content/Olentzero'\n",
        "lora_name = 'olentzero'\n",
        "batch_size = 1\n",
        "training_steps = 500\n",
        "name_or_path = \"black-forest-labs/FLUX.1-schnell\"\n",
        "width = 1024\n",
        "height = 1024\n",
        "sample_steps = 20\n",
        "\n",
        "yaml = f\"\"\"---\n",
        "job: extension\n",
        "config:\n",
        "  # this name will be the folder and filename name\n",
        "  name: {lora_name}\n",
        "  process:\n",
        "    - type: 'sd_trainer'\n",
        "      # root folder to save training sessions/samples/weights\n",
        "      training_folder: \"output\"\n",
        "      # uncomment to see performance stats in the terminal every N steps\n",
        "#      performance_log_every: 1000\n",
        "      device: cuda:0\n",
        "      # if a trigger word is specified, it will be added to captions of training data if it does not already exist\n",
        "      # alternatively, in your captions you can add [trigger] and it will be replaced with the trigger word\n",
        "#      trigger_word: \"p3r5on\"\n",
        "      network:\n",
        "        type: \"lora\"\n",
        "        linear: 16\n",
        "        linear_alpha: 16\n",
        "      save:\n",
        "        dtype: float16 # precision to save\n",
        "        save_every: 250 # save every this many steps\n",
        "        max_step_saves_to_keep: 4 # how many intermittent saves to keep\n",
        "      datasets:\n",
        "        # datasets are a folder of images. captions need to be txt files with the same name as the image\n",
        "        # for instance image2.jpg and image2.txt. Only jpg, jpeg, and png are supported currently\n",
        "        # images will automatically be resized and bucketed into the resolution specified\n",
        "        # on windows, escape back slashes with another backslash so\n",
        "        # \"C:\\\\path\\\\to\\\\images\\\\folder\"\n",
        "        - folder_path: {your_dir}\n",
        "          caption_ext: \"txt\"\n",
        "          caption_dropout_rate: 0.05  # will drop out the caption 5% of time\n",
        "          shuffle_tokens: false  # shuffle caption order, split by commas\n",
        "          cache_latents_to_disk: true  # leave this true unless you know what you're doing\n",
        "          resolution: [1024]  # flux enjoys multiple resolutions\n",
        "      train:\n",
        "        batch_size: {batch_size}\n",
        "        steps: {training_steps}  # total number of steps to train 500 - 4000 is a good range\n",
        "        gradient_accumulation_steps: 1\n",
        "        train_unet: true\n",
        "        train_text_encoder: false  # probably won't work with flux\n",
        "        gradient_checkpointing: true  # need the on unless you have a ton of vram\n",
        "        noise_scheduler: \"flowmatch\" # for training only\n",
        "        optimizer: \"adamw8bit\"\n",
        "        lr: 1e-4\n",
        "        # uncomment this to skip the pre training sample\n",
        "#        skip_first_sample: true\n",
        "        # uncomment to completely disable sampling\n",
        "#        disable_sampling: true\n",
        "        # uncomment to use new vell curved weighting. Experimental but may produce better results\n",
        "        linear_timesteps: true\n",
        "\n",
        "        # ema will smooth out learning, but could slow it down. Recommended to leave on.\n",
        "        ema_config:\n",
        "          use_ema: true\n",
        "          ema_decay: 0.99\n",
        "\n",
        "        # will probably need this if gpu supports it for flux, other dtypes may not work correctly\n",
        "        dtype: bf16\n",
        "      model:\n",
        "        # huggingface model name or path\n",
        "        name_or_path: {name_or_path}\n",
        "        is_flux: true\n",
        "        quantize: true  # run 8bit mixed precision\n",
        "#        low_vram: true  # uncomment this if the GPU is connected to your monitors. It will use less vram to quantize, but is slower.\n",
        "      sample:\n",
        "        sampler: \"flowmatch\" # must match train.noise_scheduler\n",
        "        sample_every: 250 # sample every this many steps\n",
        "        width: {width}\n",
        "        height: {height}\n",
        "        prompts:\n",
        "          - \"Olentzero\"\n",
        "          - \"A drawing of olentzero\"\n",
        "          - \"Fat Olentzero\"\n",
        "          - \"A car drived for Olentzero\"\n",
        "        neg: \"\"  # not used on flux\n",
        "        seed: 42\n",
        "        walk_seed: true\n",
        "        guidance_scale: 4\n",
        "        sample_steps: {sample_steps}\n",
        "# you can add any additional meta info here. [name] is replaced with config name at top\n",
        "meta:\n",
        "  name: \"[name]\"\n",
        "  version: '1.0'\n",
        "\"\"\"\n",
        "\n",
        "with open('/content/mylora.yaml', 'w') as file:\n",
        "    file.write(yaml)\n",
        "    file.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "869aa836-1053-4983-90e2-d4dee0f43f69",
      "metadata": {
        "id": "869aa836-1053-4983-90e2-d4dee0f43f69"
      },
      "source": [
        "## Training the Model\n",
        "\n",
        "Once everything is configured, you can train your LoRA model. Use the following command in your terminal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "de891fc3-979d-4ca7-bb42-952210930342",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-21T21:42:32.466074Z",
          "iopub.status.busy": "2024-08-21T21:42:32.465480Z",
          "iopub.status.idle": "2024-08-21T21:42:32.468786Z",
          "shell.execute_reply": "2024-08-21T21:42:32.468248Z",
          "shell.execute_reply.started": "2024-08-21T21:42:32.466050Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de891fc3-979d-4ca7-bb42-952210930342",
        "outputId": "ab7e9b01-1b7d-4f8d-d3ad-03c5fc3bfaa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running 1 job\n",
            "2024-12-05 11:50:43.788856: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-05 11:50:43.824246: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-05 11:50:43.834738: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-05 11:50:43.856887: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-05 11:50:45.407617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
            "  check_for_updates()\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/mediapipe_face/mediapipe_face_common.py:7: UserWarning: The module 'mediapipe' is not installed. The package will have limited functionality. Please install it using the command: pip install 'mediapipe'\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_5m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_5m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_11m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_11m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_224 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_224. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_384 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_384. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "/usr/local/lib/python3.10/dist-packages/controlnet_aux/segment_anything/modeling/tiny_vit_sam.py:654: UserWarning: Overwriting tiny_vit_21m_512 in registry with controlnet_aux.segment_anything.modeling.tiny_vit_sam.tiny_vit_21m_512. This is because the name being registered conflicts with an existing name. Please check if this is not expected.\n",
            "  return register_model(fn_wrapper)\n",
            "{\n",
            "    \"type\": \"sd_trainer\",\n",
            "    \"training_folder\": \"output\",\n",
            "    \"device\": \"cuda:0\",\n",
            "    \"network\": {\n",
            "        \"type\": \"lora\",\n",
            "        \"linear\": 16,\n",
            "        \"linear_alpha\": 16\n",
            "    },\n",
            "    \"save\": {\n",
            "        \"dtype\": \"float16\",\n",
            "        \"save_every\": 250,\n",
            "        \"max_step_saves_to_keep\": 4\n",
            "    },\n",
            "    \"datasets\": [\n",
            "        {\n",
            "            \"folder_path\": \"/content/Olentzero\",\n",
            "            \"caption_ext\": \"txt\",\n",
            "            \"caption_dropout_rate\": 0.05,\n",
            "            \"shuffle_tokens\": false,\n",
            "            \"cache_latents_to_disk\": true,\n",
            "            \"resolution\": [\n",
            "                1024\n",
            "            ]\n",
            "        }\n",
            "    ],\n",
            "    \"train\": {\n",
            "        \"batch_size\": 1,\n",
            "        \"steps\": 500,\n",
            "        \"gradient_accumulation_steps\": 1,\n",
            "        \"train_unet\": true,\n",
            "        \"train_text_encoder\": false,\n",
            "        \"gradient_checkpointing\": true,\n",
            "        \"noise_scheduler\": \"flowmatch\",\n",
            "        \"optimizer\": \"adamw8bit\",\n",
            "        \"lr\": 0.0001,\n",
            "        \"linear_timesteps\": true,\n",
            "        \"ema_config\": {\n",
            "            \"use_ema\": true,\n",
            "            \"ema_decay\": 0.99\n",
            "        },\n",
            "        \"dtype\": \"bf16\"\n",
            "    },\n",
            "    \"model\": {\n",
            "        \"name_or_path\": \"black-forest-labs/FLUX.1-schnell\",\n",
            "        \"is_flux\": true,\n",
            "        \"quantize\": true\n",
            "    },\n",
            "    \"sample\": {\n",
            "        \"sampler\": \"flowmatch\",\n",
            "        \"sample_every\": 250,\n",
            "        \"width\": 1024,\n",
            "        \"height\": 1024,\n",
            "        \"prompts\": [\n",
            "            \"Olentzero\",\n",
            "            \"A drawing of olentzero\",\n",
            "            \"Fat Olentzero\",\n",
            "            \"A car drived for Olentzero\"\n",
            "        ],\n",
            "        \"neg\": \"\",\n",
            "        \"seed\": 42,\n",
            "        \"walk_seed\": true,\n",
            "        \"guidance_scale\": 4,\n",
            "        \"sample_steps\": 20\n",
            "    }\n",
            "}\n",
            "Using EMA\n",
            "/content/ai-toolkit/extensions_built_in/sd_trainer/SDTrainer.py:61: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler()\n",
            "\n",
            "#############################################\n",
            "# Running job: olentzero\n",
            "#############################################\n",
            "\n",
            "\n",
            "Running  1 process\n",
            "Loading Flux model\n",
            "Loading transformer\n",
            "transformer/config.json: 100% 321/321 [00:00<00:00, 2.30MB/s]\n",
            "(…)ion_pytorch_model.safetensors.index.json: 100% 121k/121k [00:00<00:00, 1.79MB/s]\n",
            "(…)pytorch_model-00001-of-00003.safetensors: 100% 9.96G/9.96G [02:34<00:00, 64.6MB/s]\n",
            "(…)pytorch_model-00002-of-00003.safetensors: 100% 9.95G/9.95G [01:46<00:00, 93.2MB/s]\n",
            "(…)pytorch_model-00003-of-00003.safetensors: 100% 3.87G/3.87G [00:53<00:00, 71.8MB/s]\n",
            "Error running job: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 43.06 MiB is free. Process 134733 has 14.70 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 17.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "\n",
            "========================================\n",
            "Result:\n",
            " - 0 completed jobs\n",
            " - 1 failure\n",
            "========================================\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ai-toolkit/run.py\", line 90, in <module>\n",
            "    main()\n",
            "  File \"/content/ai-toolkit/run.py\", line 86, in main\n",
            "    raise e\n",
            "  File \"/content/ai-toolkit/run.py\", line 78, in main\n",
            "    job.run()\n",
            "  File \"/content/ai-toolkit/jobs/ExtensionJob.py\", line 22, in run\n",
            "    process.run()\n",
            "  File \"/content/ai-toolkit/jobs/process/BaseSDTrainProcess.py\", line 1268, in run\n",
            "    self.sd.load_model()\n",
            "  File \"/content/ai-toolkit/toolkit/stable_diffusion_model.py\", line 555, in load_model\n",
            "    transformer.to(torch.device(self.quantize_device), dtype=dtype)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/diffusers/models/modeling_utils.py\", line 1031, in to\n",
            "    return super().to(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1340, in to\n",
            "    return self._apply(convert)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 900, in _apply\n",
            "    module._apply(fn)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 927, in _apply\n",
            "    param_applied = fn(param)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1326, in convert\n",
            "    return t.to(\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 90.00 MiB. GPU 0 has a total capacity of 14.75 GiB of which 43.06 MiB is free. Process 134733 has 14.70 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 17.08 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
          ]
        }
      ],
      "source": [
        " !python3 /content/ai-toolkit/run.py /content/mylora.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2677f93-0f7b-4431-b07a-1d91b45be398",
      "metadata": {
        "tags": [],
        "id": "c2677f93-0f7b-4431-b07a-1d91b45be398"
      },
      "source": [
        "## Running Inference with the New Model ( FLUX.1 LoRA)\n",
        "\n",
        "After training, you can run inference to generate images based on the trained model. Ensure you have the necessary packages installed:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4293b7e6",
      "metadata": {
        "id": "4293b7e6"
      },
      "outputs": [],
      "source": [
        "!pip install -U diffusers accelerate transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bc308d4",
      "metadata": {
        "id": "1bc308d4"
      },
      "source": [
        "\n",
        "Then, use the following code to load the model and generate an image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7f0bbde-6764-42d7-b616-df90f2842e3b",
      "metadata": {
        "id": "d7f0bbde-6764-42d7-b616-df90f2842e3b"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from diffusers import DiffusionPipeline\n",
        "\n",
        "model_id = 'black-forest-labs/FLUX.1-schnell'\n",
        "adapter_id = f'output/{lora_name}/{lora_name}.safetensors'\n",
        "pipeline = DiffusionPipeline.from_pretrained(model_id)\n",
        "pipeline.load_lora_weights(adapter_id)\n",
        "\n",
        "prompt = \"Olentzero drinking orange juice\"\n",
        "negative_prompt = \"blurry, cropped, ugly\"\n",
        "\n",
        "pipeline.to('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
        "image = pipeline(\n",
        "    prompt=prompt,\n",
        "    num_inference_steps=50,\n",
        "    generator=torch.Generator(device='cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').manual_seed(1641421826),\n",
        "    width=1152,\n",
        "    height=768,\n",
        ").images[0]\n",
        "# image.save(\"output.png\", format=\"PNG\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed51fe37",
      "metadata": {
        "id": "ed51fe37"
      },
      "source": [
        "\n",
        "Finally, display the generated image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcf99335-95d8-4456-a2b0-c7dbe223604d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-08-21T21:28:12.539986Z",
          "iopub.status.busy": "2024-08-21T21:28:12.539616Z",
          "iopub.status.idle": "2024-08-21T21:28:12.542376Z",
          "shell.execute_reply": "2024-08-21T21:28:12.541939Z",
          "shell.execute_reply.started": "2024-08-21T21:28:12.539964Z"
        },
        "tags": [],
        "id": "bcf99335-95d8-4456-a2b0-c7dbe223604d"
      },
      "outputs": [],
      "source": [
        "display(image)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}